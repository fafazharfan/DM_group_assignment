{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DM_GroupAssignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HULW0YTFhZa6",
        "outputId": "6a45a5cc-fae4-4573-a402-4e37388cafa6"
      },
      "source": [
        "!pip install spacy==2.3.5\n",
        "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz\n",
        "import re, nltk, spacy, gensim\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import ToktokTokenizer\n",
        "from nltk.stem import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from string import punctuation\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.manifold import TSNE\n",
        "from keras.metrics import top_k_categorical_accuracy\n",
        "from keras.optimizers import adam_v2\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Conv1D, MaxPooling1D, Flatten\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import pandas as pd\n",
        "import string\n",
        "import itertools\n",
        "import io\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy==2.3.5 in /usr/local/lib/python3.7/dist-packages (2.3.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (1.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (1.21.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (4.64.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (3.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (2.0.6)\n",
            "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (7.4.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (0.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (57.4.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.3.5) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.3.5) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.3.5) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.3.5) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (1.24.3)\n",
            "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 9.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==2.3.1) (2.3.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (1.1.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (0.4.1)\n",
            "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (7.4.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (57.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (0.9.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (1.21.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (3.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (1.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (2.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (1.0.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (2021.10.8)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4f0f6e0-9635-4e0d-89c2-4f74dbec2091",
        "id": "R-tTO-sDVrkE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/\"\n",
        "pubmed_database = path + \"pubmed-diabetesme-set.txt\"\n",
        "\n",
        "#df = pd.read_csv(pubmed_database, sep=\" -\", header=None, \n",
        "                # names=[\"Terms\", \"Info\"])\n",
        "\n",
        "#display (df)"
      ],
      "metadata": {
        "id": "VyjNzk0kL2kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_PMID_Title_Tabstract_and_mh(lines):\n",
        "  s = \"\".join(lines)\n",
        "  s = re.sub(\"\\n\\s+\", \"\", s)\n",
        "  s = s.strip()\n",
        "  s = s.split(\"\\n\")\n",
        "\n",
        "  title = {}\n",
        "  abstract = \"\"\n",
        "  mh = []\n",
        "\n",
        "  for line in s:\n",
        "    if line[0:5]==\"TI  -\":\n",
        "      title = line[6:]\n",
        "    elif line[0:5]==\"AB  -\":\n",
        "      abstract = line[6:]\n",
        "    elif line[0:5]==\"MH  -\":\n",
        "      mh.append(line[6:])\n",
        "  return [title,abstract, mh]"
      ],
      "metadata": {
        "id": "Dvr7RCcYPbym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "records = []\n",
        "for i, p in enumerate(\"pubmed-diabetesme-set.txt\"):\n",
        "  f = open(pubmed_database, \"r\")\n",
        "  temp_record = []\n",
        "  r = []\n",
        "  for line in f:\n",
        "    if line != \"\\n\":\n",
        "      r.append(line)\n",
        "    else :\n",
        "      temp_record.append(get_PMID_Title_Tabstract_and_mh(r))\n",
        "      r = []\n",
        "  f.close()\n",
        "  records.append(temp_record)"
      ],
      "metadata": {
        "id": "Kij7bRlfgv_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "records[1][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGNMZCARSC_Y",
        "outputId": "f9ddffa5-1fdc-4e89-bfda-3d00f013bf23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"India is one of the epicentres of the global diabetes mellitus pandemic. Rapid socioeconomic development and demographic changes, along with increased susceptibility for Indian individuals, have led to the explosive increase in the prevalence of diabetes mellitus in India over the past four decades. Type 2 diabetes mellitus in Asian Indian people is characterized by a young age of onset and occurrence at low levels of BMI. Available data also suggest that the susceptibility of Asian Indian people to the complications of diabetes mellitus differs from that of white populations. Management of this disease in India faces multiple challenges, such as low levels of awareness, paucity of trained medical and paramedical staff and unaffordability of medications and services. Novel interventions using readily available resources and technology promise to revolutionise the care of patients with diabetes mellitus in India. As many of these challenges are common to most developing countries of the world, the lessons learnt from India's experience with diabetes mellitus are likely to be of immense global relevance. In this Review, we discuss the epidemiology of diabetes mellitus and its complications in India and outline the advances made in the country to ensure adequate care. We make specific references to novel, cost-effective interventions, which might be of relevance to other low-income and middle-income countries of the world.\",\n",
              " ['*Developing Countries',\n",
              "  'Diabetes Complications/*epidemiology/etiology/therapy',\n",
              "  'Diabetes Mellitus, Type 1/complications/epidemiology/therapy',\n",
              "  'Diabetes Mellitus, Type 2/complications/*epidemiology/therapy',\n",
              "  'Diabetes, Gestational/epidemiology/therapy',\n",
              "  'Female',\n",
              "  'Health Care Costs',\n",
              "  'Humans',\n",
              "  'India/epidemiology',\n",
              "  'Male',\n",
              "  'Pregnancy',\n",
              "  'Prevalence']]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token = ToktokTokenizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "punct = punctuation\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(\"\\d+\", \"\", text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"\\'\\n\", \" \", text)\n",
        "    text = re.sub(r\"\\'\\xa0\", \" \", text)\n",
        "    text = re.sub('\\s+', ' ', text) # matches all whitespace characters\n",
        "    text = text.strip(' ')\n",
        "    return text\n",
        "  \n",
        "def strip_list_noempty(mylist):\n",
        "    \n",
        "    newlist = (item.strip() if hasattr(item, 'strip') else item for item in mylist)\n",
        "    return [item for item in newlist if item != '']\n",
        "    \n",
        "    \n",
        "def clean_punct(text): \n",
        "    ''' Remove punctuations'''\n",
        "    words = token.tokenize(text)\n",
        "    punctuation_filtered = []\n",
        "    regex = re.compile('[%s]' % re.escape(punct))\n",
        "    remove_punctuation = str.maketrans(' ', ' ', punct)\n",
        "    \n",
        "    for w in words:\n",
        "        w = re.sub('^[0-9]*', \" \", w)\n",
        "        punctuation_filtered.append(regex.sub('', w))\n",
        "  \n",
        "    filtered_list = strip_list_noempty(punctuation_filtered)\n",
        "        \n",
        "    return ' '.join(map(str, filtered_list))\n",
        "  \n",
        "def stopWordsRemove(text):\n",
        "    words = token.tokenize(text)\n",
        "    filtered = [w for w in words if not w in stop_words]\n",
        "    \n",
        "    return ' '.join(map(str, filtered))\n",
        "  \n",
        "def lemmatization(texts, allowed_postags, stop_words=stop_words):\n",
        "    lemma = wordnet.WordNetLemmatizer()       \n",
        "    doc = nlp(texts) \n",
        "    texts_out = []\n",
        "    \n",
        "    for token in doc:\n",
        "        if token.pos_ in allowed_postags:\n",
        "            \n",
        "          if token.lemma_ not in ['-PRON-']:\n",
        "              texts_out.append(token.lemma_)\n",
        "              \n",
        "          else:\n",
        "              texts_out.append('')\n",
        "     \n",
        "    texts_out = ' '.join(texts_out)\n",
        "\n",
        "    return texts_out"
      ],
      "metadata": {
        "id": "yeZiWP5zUT3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for r in records:\n",
        "  for d in r:\n",
        "    d[0] = clean_text(d[0])\n",
        "    d[0] = BeautifulSoup(d[0]).get_text()\n",
        "    d[0] = clean_punct(d[0])\n",
        "    d[0] = stopWordsRemove(d[0])"
      ],
      "metadata": {
        "id": "NQ3pO9xoUYv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "records_combined = list(itertools.chain.from_iterable(records))"
      ],
      "metadata": {
        "id": "M5Jf_YJoZhMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_v_all_learning(records):\n",
        "  X = []\n",
        "  y = []\n",
        "  for d in records:\n",
        "    X.append(d[1])\n",
        "    y.append(d[0])\n",
        "  X = np.array(X)\n",
        "  y = np.array(y)\n",
        "  # Sampling dataset\n",
        "  vectorizer_X = TfidfVectorizer(analyzer='word', min_df=0.0, max_df = 1.0, \n",
        "                                    strip_accents = None, encoding = 'utf-8', \n",
        "                                    preprocessor=None, \n",
        "                                    token_pattern=r\"(?u)\\S\\S+\", # Need to repeat token pattern\n",
        "                                    max_features=1000)\n",
        "  # 80/20 split\n",
        "  X_train, X_test, y_train, y_test = train_test_split(\n",
        "      X, y, test_size=0.2,train_size=0.8, random_state=0)\n",
        "\n",
        "  # TF-IDF matrices\n",
        "  X_tfidf_train = vectorizer_X.fit_transform(X_train)\n",
        "  X_tfidf_test = vectorizer_X.transform(X_test)\n",
        "\n",
        "  #dimensionality reduction dengan PCA\n",
        "  pca = PCA(n_components=0.95)\n",
        "  X_tfidf_train = pca.fit_transform(X_tfidf_train.todense())\n",
        "  X_tfidf_test = pca.transform(X_tfidf_test.todense())\n",
        "\n",
        "  mh_list = []\n",
        "  for d in records:\n",
        "    mh_list += d[1]\n",
        "  values, counts = np.unique(mh_list, return_counts=True)\n",
        "  ind = (-counts).argsort()[:10]\n",
        "  top_tags = values[ind]\n",
        "  print(top_tags)\n",
        "  reports = {}\n",
        "  for i in range(len(top_tags)):\n",
        "    y_train_temp = [1 if top_tags[i] in row else 0 for row in y_train]\n",
        "    y_test_temp = [1 if top_tags[i] in row else 0 for row in y_test]\n",
        "    clf = LogisticRegression(penalty= 'elasticnet', solver= 'saga', l1_ratio=0.5, random_state=1)\n",
        "    clf.fit(X_tfidf_train, y_train_temp)\n",
        "    preds = clf.predict(X_tfidf_test)\n",
        "    print(\"Hasil Klasifikasi Kelas {}\".format(top_tags[i]))\n",
        "    print(classification_report(y_test_temp, preds))\n",
        "    reports[top_tags[i]] = classification_report(y_test_temp, preds, output_dict=True)\n",
        "  return reports"
      ],
      "metadata": {
        "id": "BTXZ4_6Gd7ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logreg_report_combined = one_v_all_learning(records_combined)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ji7a7HVyeG3q",
        "outputId": "925e3878-2884-4d71-ee5e-4f179cc2c3fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  import sys\n"
          ]
        }
      ]
    }
  ]
}